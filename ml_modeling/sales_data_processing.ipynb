{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Modelling\n",
    "This is a notebook to experiment with the data modelling of the sales quantity data.\n",
    "This was done on a cloud instance so the file paths will be different if you are running this locally.\n",
    "Note that the dataset is also propiertary so it will not be included in this repository."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.Imports and Constants\n",
    "We will be using the following libraries:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 13:17:43.465122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-11 13:17:44.357801: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-06-11 13:17:44.357915: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-06-11 13:17:44.357924: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "id": "aryZ62neeLK9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686287225110,
     "user_tz": -420,
     "elapsed": 3548,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-06-11T13:17:49.266127500Z",
     "start_time": "2023-06-11T13:17:42.951228900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "WINDOW = 20\n",
    "BATCH_SIZE = 2048\n",
    "BUFFER = 100000"
   ],
   "metadata": {
    "id": "p8TxgaNyrSqD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686287225115,
     "user_tz": -420,
     "elapsed": 41,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {
    "collapsed": false,
    "id": "zHaCTqUKeLK_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data is a csv extracted from an SQL database and cleaned. It contains the following columns:\n",
    "- **date:** The date of the sale\n",
    "- **item_code:** The code of the product sold\n",
    "- **quantity:** The quantity of the product sold on that day"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %pwd"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "GwN8M6XleqWL",
    "executionInfo": {
     "status": "error",
     "timestamp": 1686285433739,
     "user_tz": -420,
     "elapsed": 553,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "92a58a81-65a9-484d-ee8d-24da43af0de5"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "         date      item_code  quantity\n0  2022-08-26       00001000        15\n1  2022-08-26       00000500        14\n2  2023-01-01  8991102380706        13\n3  2023-01-01  8991102381017        13\n4  2023-01-01  8886008101053        20",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>item_code</th>\n      <th>quantity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2022-08-26</td>\n      <td>00001000</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022-08-26</td>\n      <td>00000500</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2023-01-01</td>\n      <td>8991102380706</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2023-01-01</td>\n      <td>8991102381017</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2023-01-01</td>\n      <td>8886008101053</td>\n      <td>20</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filepath = 'sales_quantity.csv' #for local imports\n",
    "filepath = '/home/mariefloco/sales_quantity.csv' #colab\n",
    "data = pd.read_csv(filepath,names=['date','item_code','quantity'],header = 0 )\n",
    "data.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "lP40rLWweLLA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686287226767,
     "user_tz": -420,
     "elapsed": 1689,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "4b1e67e9-1f43-4e30-ed74-aaa1a0de72ab"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transform data\n",
    "We need to change the data so that it has the following format for training:\n",
    "- **Input:** [*The tokenized item code, day, month, day of the week, day of the year, {A sequence of 20 days of sales data for a particular product}*]\n",
    "- **Output:** The quantity sold for that product in the following day\n",
    ">**Note:**\n",
    "    - *The tokenized item code is the index of the item code in the tokenizer's word index.*\n",
    "    - *The day component of the date is the day of the month.*\n",
    "    - *The month component of the date is the month of the year.*\n",
    "    - *The day of the week is a number between 0 and 6, where 0 is Monday and 6 is Sunday.*\n",
    "    - *The day of the year is a number between 1 and 365, where January 1st is 1 and December 31st is 365.*\n",
    "    - *The sequence of 20 days of sales data is the window size we will use for training the model. The data will be normalized*\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#extract date features from date column\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day\n",
    "data['day_of_week'] = data['date'].dt.dayofweek\n",
    "data['day_of_year'] = data['date'].dt.dayofyear\n",
    "\n"
   ],
   "metadata": {
    "id": "RtmLJm2MeLLC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686287226768,
     "user_tz": -420,
     "elapsed": 15,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to create a wide dataframe with each item code as a column and the quantity sold for each day as the values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "item_code       date  year  month  day  day_of_week  day_of_year  \\\n0         2022-01-03  2022      1    3            0            3   \n1         2022-01-04  2022      1    4            1            4   \n2         2022-01-05  2022      1    5            2            5   \n3         2022-01-06  2022      1    6            3            6   \n4         2022-01-07  2022      1    7            4            7   \n..               ...   ...    ...  ...          ...          ...   \n446       2023-03-28  2023      3   28            1           87   \n447       2023-03-29  2023      3   29            2           88   \n448       2023-03-30  2023      3   30            3           89   \n449       2023-03-31  2023      3   31            4           90   \n450       2023-04-01  2023      4    1            5           91   \n\nitem_code  (90)NA18210500154(91)2403  (90)NA18211207820(91)2410  00000001  \\\n0                                0.0                        0.0       0.0   \n1                                0.0                        0.0       0.0   \n2                                0.0                        0.0       0.0   \n3                                0.0                        0.0       0.0   \n4                                0.0                        0.0       0.0   \n..                               ...                        ...       ...   \n446                              0.0                        0.0       0.0   \n447                              0.0                        0.0       0.0   \n448                              0.0                        0.0       0.0   \n449                              0.0                        0.0       0.0   \n450                              0.0                        0.0       0.0   \n\nitem_code  00000002  ...  CL000448327  CL000450943  COS LT  COSLT-228  \\\n0               0.0  ...          0.0          0.0     0.0        0.0   \n1               0.0  ...          0.0          0.0     0.0        0.0   \n2               0.0  ...          0.0          0.0     0.0        1.0   \n3               0.0  ...          0.0          0.0     0.0        0.0   \n4               0.0  ...          0.0          0.0     0.0        0.0   \n..              ...  ...          ...          ...     ...        ...   \n446             0.0  ...          0.0          0.0     0.0        1.0   \n447             0.0  ...          0.0          0.0     0.0        2.0   \n448             0.0  ...          0.0          0.0     0.0        0.0   \n449             0.0  ...          0.0          0.0     0.0        0.0   \n450             0.0  ...          0.0          0.0     0.0        0.0   \n\nitem_code  EC0102190002  EC0102191301  EC0103190002  EC0106190101  MP-2203  \\\n0                   0.0           0.0           0.0           0.0      0.0   \n1                   0.0           0.0           0.0           0.0      0.0   \n2                   0.0           0.0           0.0           0.0      0.0   \n3                   0.0           0.0           0.0           0.0      0.0   \n4                   0.0           0.0           0.0           0.0      0.0   \n..                  ...           ...           ...           ...      ...   \n446                 0.0           0.0           0.0           0.0      0.0   \n447                 0.0           0.0           0.0           0.0      0.0   \n448                 0.0           0.0           0.0           0.0      0.0   \n449                 0.0           0.0           0.0           0.0      0.0   \n450                 0.0           0.0           0.0           0.0      0.0   \n\nitem_code  SLM0958266  \n0                 0.0  \n1                 0.0  \n2                 0.0  \n3                 0.0  \n4                 0.0  \n..                ...  \n446               0.0  \n447               0.0  \n448               0.0  \n449               0.0  \n450               0.0  \n\n[451 rows x 13923 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>item_code</th>\n      <th>date</th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>day_of_week</th>\n      <th>day_of_year</th>\n      <th>(90)NA18210500154(91)2403</th>\n      <th>(90)NA18211207820(91)2410</th>\n      <th>00000001</th>\n      <th>00000002</th>\n      <th>...</th>\n      <th>CL000448327</th>\n      <th>CL000450943</th>\n      <th>COS LT</th>\n      <th>COSLT-228</th>\n      <th>EC0102190002</th>\n      <th>EC0102191301</th>\n      <th>EC0103190002</th>\n      <th>EC0106190101</th>\n      <th>MP-2203</th>\n      <th>SLM0958266</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2022-01-03</td>\n      <td>2022</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022-01-04</td>\n      <td>2022</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2022-01-05</td>\n      <td>2022</td>\n      <td>1</td>\n      <td>5</td>\n      <td>2</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2022-01-06</td>\n      <td>2022</td>\n      <td>1</td>\n      <td>6</td>\n      <td>3</td>\n      <td>6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2022-01-07</td>\n      <td>2022</td>\n      <td>1</td>\n      <td>7</td>\n      <td>4</td>\n      <td>7</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>446</th>\n      <td>2023-03-28</td>\n      <td>2023</td>\n      <td>3</td>\n      <td>28</td>\n      <td>1</td>\n      <td>87</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>447</th>\n      <td>2023-03-29</td>\n      <td>2023</td>\n      <td>3</td>\n      <td>29</td>\n      <td>2</td>\n      <td>88</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>448</th>\n      <td>2023-03-30</td>\n      <td>2023</td>\n      <td>3</td>\n      <td>30</td>\n      <td>3</td>\n      <td>89</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>449</th>\n      <td>2023-03-31</td>\n      <td>2023</td>\n      <td>3</td>\n      <td>31</td>\n      <td>4</td>\n      <td>90</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>450</th>\n      <td>2023-04-01</td>\n      <td>2023</td>\n      <td>4</td>\n      <td>1</td>\n      <td>5</td>\n      <td>91</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>451 rows Ã— 13923 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stack dataframe based on item_code\n",
    "item_sales = data.groupby(['item_code','date','year','month','day','day_of_week','day_of_year'])['quantity'].sum().unstack(level=0)\n",
    "#turn each NaN value to 0\n",
    "item_sales = item_sales.fillna(0)\n",
    "item_sales.reset_index(inplace=True)\n",
    "item_sales.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "7xKluIgqeLLD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686287226769,
     "user_tz": -420,
     "elapsed": 16,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "a50777a9-4d54-4691-80ea-735c48a7de9b"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we will be passing the item code as a feature to the model, we need to tokenize it.\n",
    "We use a helper function to create a tokenizer and fit it on the item codes.\n",
    "This will be also be used to later decode the predictions of the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer has 13917 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'(90)NA18210500154(91)2403': 1,\n '(90)NA18211207820(91)2410': 2,\n '00000001': 3,\n '00000002': 4,\n '00000003': 5,\n '00000008': 6,\n '00000010': 7,\n '00000011': 8,\n '00000012': 9,\n '00000013': 10,\n '00000014': 11,\n '00000015': 12,\n '00000016': 13,\n '00000017': 14,\n '00000019': 15,\n '00000020': 16,\n '00000021': 17,\n '00000022': 18,\n '00000023': 19,\n '00000024': 20,\n '00000025': 21,\n '00000026': 22,\n '00000027': 23,\n '00000030': 24,\n '00000031': 25,\n '00000032': 26,\n '00000034': 27,\n '00000035': 28,\n '00000036': 29,\n '00000037': 30,\n '00000038': 31,\n '00000039': 32,\n '00000040': 33,\n '00000041': 34,\n '00000042': 35,\n '00000044': 36,\n '00000045': 37,\n '00000046': 38,\n '00000047': 39,\n '00000048': 40,\n '00000049': 41,\n '00000050': 42,\n '00000051': 43,\n '00000052': 44,\n '00000054': 45,\n '00000057': 46,\n '00000058': 47,\n '00000059': 48,\n '00000060': 49,\n '00000061': 50,\n '00000062': 51,\n '00000063': 52,\n '00000064': 53,\n '00000065': 54,\n '00000067': 55,\n '00000069': 56,\n '00000070': 57,\n '00000071': 58,\n '00000073': 59,\n '00000074': 60,\n '00000075': 61,\n '00000076': 62,\n '00000077': 63,\n '00000078': 64,\n '00000080': 65,\n '00000081': 66,\n '00000083': 67,\n '00000084': 68,\n '00000085': 69,\n '00000088': 70,\n '00000089': 71,\n '00000090': 72,\n '00000091': 73,\n '00000092': 74,\n '00000093': 75,\n '00000094': 76,\n '00000095': 77,\n '00000096': 78,\n '00000102': 79,\n '00000103': 80,\n '00000108': 81,\n '00000109': 82,\n '00000112': 83,\n '00000113': 84,\n '00000116': 85,\n '00000117': 86,\n '00000119': 87,\n '00000120': 88,\n '00000121': 89,\n '00000122': 90,\n '00000123': 91,\n '00000125': 92,\n '00000128': 93,\n '00000129': 94,\n '00000130': 95,\n '00000134': 96,\n '00000136': 97,\n '00000138': 98,\n '00000139': 99,\n '00000142': 100,\n '00000144': 101,\n '00000148': 102,\n '00000149': 103,\n '00000153': 104,\n '00000157': 105,\n '00000160': 106,\n '00000166': 107,\n '000001663': 108,\n '00000167': 109,\n '00000171': 110,\n '00000173': 111,\n '00000182': 112,\n '00000184': 113,\n '00000206': 114,\n '00000207': 115,\n '00000209': 116,\n '00000210': 117,\n '00000211': 118,\n '00000215': 119,\n '00000220': 120,\n '00000221': 121,\n '00000223': 122,\n '00000224': 123,\n '00000225': 124,\n '00000227': 125,\n '00000228': 126,\n '00000231': 127,\n '00000233': 128,\n '00000234': 129,\n '00000236': 130,\n '00000237': 131,\n '000002387': 132,\n '00000240': 133,\n '00000243': 134,\n '00000244': 135,\n '00000245': 136,\n '00000246': 137,\n '00000247': 138,\n '00000250': 139,\n '00000254': 140,\n '00000264': 141,\n '00000267': 142,\n '00000269': 143,\n '00000270': 144,\n '00000271': 145,\n '00000272': 146,\n '00000273': 147,\n '00000274': 148,\n '00000278': 149,\n '00000280': 150,\n '00000281': 151,\n '00000282': 152,\n '00000283': 153,\n '00000286': 154,\n '00000288': 155,\n '00000290': 156,\n '00000296': 157,\n '00000297': 158,\n '00000301': 159,\n '00000302': 160,\n '00000303': 161,\n '00000304': 162,\n '00000307': 163,\n '00000310': 164,\n '00000313': 165,\n '00000318': 166,\n '00000320': 167,\n '00000322': 168,\n '00000323': 169,\n '00000324': 170,\n '00000325': 171,\n '00000326': 172,\n '00000328': 173,\n '00000334': 174,\n '00000335': 175,\n '00000338': 176,\n '00000340': 177,\n '00000341': 178,\n '00000342': 179,\n '00000343': 180,\n '00000344': 181,\n '00000345': 182,\n '00000347': 183,\n '00000348': 184,\n '00000349': 185,\n '00000353': 186,\n '00000354': 187,\n '00000355': 188,\n '00000356': 189,\n '00000358': 190,\n '00000366': 191,\n '00000368': 192,\n '00000369': 193,\n '00000370': 194,\n '00000374': 195,\n '00000375': 196,\n '00000376': 197,\n '00000377': 198,\n '00000379': 199,\n '00000380': 200,\n '00000384': 201,\n '000003845': 202,\n '00000385': 203,\n '00000387': 204,\n '00000390': 205,\n '00000396': 206,\n '00000399': 207,\n '00000401': 208,\n '00000408': 209,\n '00000413': 210,\n '00000416': 211,\n '00000418': 212,\n '00000426': 213,\n '00000429': 214,\n '00000430': 215,\n '00000431': 216,\n '00000439': 217,\n '00000440': 218,\n '00000441': 219,\n '00000449': 220,\n '00000453': 221,\n '00000454': 222,\n '00000456': 223,\n '00000457': 224,\n '00000464': 225,\n '00000467': 226,\n '00000468': 227,\n '00000472': 228,\n '00000474': 229,\n '00000476': 230,\n '00000482': 231,\n '00000484': 232,\n '00000485': 233,\n '00000486': 234,\n '00000487': 235,\n '00000488': 236,\n '00000489': 237,\n '00000490': 238,\n '00000491': 239,\n '00000495': 240,\n '00000496': 241,\n '00000497': 242,\n '00000500': 243,\n '00000502': 244,\n '00000504': 245,\n '00000506': 246,\n '00000507': 247,\n '00000508': 248,\n '00000509': 249,\n '00000522': 250,\n '00000525': 251,\n '00000531': 252,\n '00000533': 253,\n '00000534': 254,\n '00000537': 255,\n '00000538': 256,\n '00000541': 257,\n '00000545': 258,\n '00000546': 259,\n '00000551': 260,\n '00000552': 261,\n '00000553': 262,\n '00000554': 263,\n '00000557': 264,\n '00000558': 265,\n '00000559': 266,\n '00000560': 267,\n '00000568': 268,\n '00000571': 269,\n '00000572': 270,\n '00000573': 271,\n '00000575': 272,\n '00000576': 273,\n '00000583': 274,\n '00000589': 275,\n '00000594': 276,\n '00000595': 277,\n '00000598': 278,\n '00000600': 279,\n '00000601': 280,\n '00000602': 281,\n '00000603': 282,\n '00000605': 283,\n '00000606': 284,\n '00000607': 285,\n '00000609': 286,\n '00000610': 287,\n '00000611': 288,\n '00000613': 289,\n '00000614': 290,\n '00000618': 291,\n '00000633': 292,\n '00000640': 293,\n '00000644': 294,\n '00000646': 295,\n '00000653': 296,\n '00000654': 297,\n '00000658': 298,\n '00000660': 299,\n '00000663': 300,\n '00000666': 301,\n '00000670': 302,\n '00000671': 303,\n '00000672': 304,\n '00000673': 305,\n '00000675': 306,\n '00000686': 307,\n '00000689': 308,\n '00000694': 309,\n '00000695': 310,\n '00000706': 311,\n '00000707': 312,\n '00000710': 313,\n '00000711': 314,\n '00000714': 315,\n '00000725': 316,\n '00000726': 317,\n '00000727': 318,\n '00000728': 319,\n '00000729': 320,\n '00000735': 321,\n '00000736': 322,\n '00000737': 323,\n '00000738': 324,\n '00000741': 325,\n '00000744': 326,\n '00000745': 327,\n '00000746': 328,\n '00000747': 329,\n '00000750': 330,\n '00000751': 331,\n '00000753': 332,\n '00000758': 333,\n '00000759': 334,\n '00000760': 335,\n '00000769': 336,\n '00000771': 337,\n '00000775': 338,\n '00000777': 339,\n '00000781': 340,\n '00000782': 341,\n '00000784': 342,\n '00000789': 343,\n '00000791': 344,\n '00000792': 345,\n '00000794': 346,\n '00000795': 347,\n '00000796': 348,\n '00000799': 349,\n '00000800': 350,\n '00000802': 351,\n '00000804': 352,\n '00000807': 353,\n '00000811': 354,\n '00000819': 355,\n '00000828': 356,\n '00000832': 357,\n '00000839': 358,\n '00000840': 359,\n '00000850': 360,\n '00000862': 361,\n '00000863': 362,\n '00000864': 363,\n '00000867': 364,\n '00000869': 365,\n '00000871': 366,\n '00000875': 367,\n '00000876': 368,\n '00000877': 369,\n '00000878': 370,\n '00000879': 371,\n '00000880': 372,\n '00000882': 373,\n '00000883': 374,\n '00000888': 375,\n '00000889': 376,\n '00000890': 377,\n '00000894': 378,\n '00000896': 379,\n '00000897': 380,\n '00000899': 381,\n '00000902': 382,\n '00000906': 383,\n '00000909': 384,\n '00000913': 385,\n '00000921': 386,\n '00000926': 387,\n '00000928': 388,\n '00000937': 389,\n '00000943': 390,\n '00000944': 391,\n '00000947': 392,\n '00000951': 393,\n '00000964': 394,\n '00000965': 395,\n '00000977': 396,\n '00000980': 397,\n '00000983': 398,\n '00000986': 399,\n '00000987': 400,\n '00000988': 401,\n '00000994': 402,\n '00001000': 403,\n '00001006': 404,\n '00001008': 405,\n '00001009': 406,\n '00001011': 407,\n '00001014': 408,\n '00001016': 409,\n '00001018': 410,\n '00001019': 411,\n '00001020': 412,\n '00001021': 413,\n '00001022': 414,\n '00001023': 415,\n '00001025': 416,\n '00001028': 417,\n '00001029': 418,\n '00001030': 419,\n '00001035': 420,\n '00001036': 421,\n '00001037': 422,\n '00001039': 423,\n '00001045': 424,\n '00001046': 425,\n '00001055': 426,\n '00001059': 427,\n '00001060': 428,\n '00001062': 429,\n '00001067': 430,\n '00001074': 431,\n '00001077': 432,\n '00001078': 433,\n '00001080': 434,\n '00001082': 435,\n '00001100': 436,\n '00001108': 437,\n '00001110': 438,\n '00001113': 439,\n '00001116': 440,\n '00001117': 441,\n '00001119': 442,\n '00001121': 443,\n '00001123': 444,\n '00001124': 445,\n '00001126': 446,\n '00001127': 447,\n '00001128': 448,\n '00001134': 449,\n '00001136': 450,\n '00001137': 451,\n '00001138': 452,\n '00001140': 453,\n '00001144': 454,\n '00001151': 455,\n '00001153': 456,\n '00001157': 457,\n '00001163': 458,\n '00001164': 459,\n '00001168': 460,\n '00001170': 461,\n '00001171': 462,\n '00001172': 463,\n '00001173': 464,\n '00001174': 465,\n '00001177': 466,\n '00001179': 467,\n '00001181': 468,\n '00001187': 469,\n '00001189': 470,\n '00001190': 471,\n '00001191': 472,\n '00001192': 473,\n '00001194': 474,\n '00001196': 475,\n '00001197': 476,\n '00001198': 477,\n '00001200': 478,\n '00001202': 479,\n '00001203': 480,\n '00001206': 481,\n '00001207': 482,\n '00001208': 483,\n '00001210': 484,\n '00001213': 485,\n '00001219': 486,\n '00001228': 487,\n '00001231': 488,\n '00001233': 489,\n '00001234': 490,\n '00001236': 491,\n '00001237': 492,\n '00001238': 493,\n '00001240': 494,\n '00001242': 495,\n '00001244': 496,\n '00001247': 497,\n '00001251': 498,\n '00001258': 499,\n '00001262': 500,\n '00001268': 501,\n '00001272': 502,\n '00001273': 503,\n '00001275': 504,\n '00001277': 505,\n '00001282': 506,\n '00001284': 507,\n '00001289': 508,\n '00001297': 509,\n '00001299': 510,\n '00001301': 511,\n '00001302': 512,\n '00001304': 513,\n '00001309': 514,\n '00001312': 515,\n '00001313': 516,\n '00001314': 517,\n '00001315': 518,\n '00001316': 519,\n '00001317': 520,\n '00001318': 521,\n '00001319': 522,\n '00001324': 523,\n '00001325': 524,\n '00001326': 525,\n '00001328': 526,\n '00001336': 527,\n '00001338': 528,\n '00001339': 529,\n '00001348': 530,\n '00001353': 531,\n '00001358': 532,\n '00001359': 533,\n '00001364': 534,\n '00001375': 535,\n '00001377': 536,\n '00001378': 537,\n '00001379': 538,\n '00001385': 539,\n '00001386': 540,\n '00001389': 541,\n '00001390': 542,\n '00001398': 543,\n '00001401': 544,\n '00001403': 545,\n '00001406': 546,\n '00001407': 547,\n '00001409': 548,\n '00001411': 549,\n '00001414': 550,\n '00001415': 551,\n '00001417': 552,\n '00001421': 553,\n '00001431': 554,\n '00001438': 555,\n '00001447': 556,\n '00001451': 557,\n '00001453': 558,\n '00001458': 559,\n '00001460': 560,\n '00001463': 561,\n '00001466': 562,\n '00001467': 563,\n '00001468': 564,\n '00001470': 565,\n '00001472': 566,\n '00001473': 567,\n '00001474': 568,\n '00001475': 569,\n '00001476': 570,\n '00001477': 571,\n '00001478': 572,\n '00001489': 573,\n '00001513': 574,\n '00001517': 575,\n '00001518': 576,\n '00001524': 577,\n '00001525': 578,\n '00001534': 579,\n '00001542': 580,\n '00001544': 581,\n '00001547': 582,\n '00001550': 583,\n '00001554': 584,\n '00001561': 585,\n '00001562': 586,\n '00001563': 587,\n '00001564': 588,\n '00001566': 589,\n '00001570': 590,\n '00001575': 591,\n '00001585': 592,\n '00001590': 593,\n '00001591': 594,\n '00001592': 595,\n '00001593': 596,\n '00001594': 597,\n '00001595': 598,\n '00001596': 599,\n '00001597': 600,\n '00001598': 601,\n '00001602': 602,\n '00001603': 603,\n '00001604': 604,\n '00001608': 605,\n '00001610': 606,\n '00001612': 607,\n '00001625': 608,\n '00001629': 609,\n '00001633': 610,\n '00001638': 611,\n '00001639': 612,\n '00001641': 613,\n '00001642': 614,\n '00001646': 615,\n '00001649': 616,\n '00001654': 617,\n '00001670': 618,\n '00001671': 619,\n '00001672': 620,\n '00001673': 621,\n '00001681': 622,\n '00001682': 623,\n '00001688': 624,\n '00001693': 625,\n '00001704': 626,\n '00001705': 627,\n '00001711': 628,\n '00001712': 629,\n '00001713': 630,\n '00001714': 631,\n '00001715': 632,\n '00001723': 633,\n '00001728': 634,\n '00001755': 635,\n '00001759': 636,\n '00001760': 637,\n '00001762': 638,\n '00001763': 639,\n '00001766': 640,\n '00001770': 641,\n '00001772': 642,\n '00001773': 643,\n '00001774': 644,\n '00001776': 645,\n '00001777': 646,\n '00001780': 647,\n '00001782': 648,\n '00001785': 649,\n '00001789': 650,\n '00001791': 651,\n '00001802': 652,\n '00001803': 653,\n '00001804': 654,\n '00001805': 655,\n '00001806': 656,\n '00001807': 657,\n '00001808': 658,\n '00001813': 659,\n '00001816': 660,\n '00001819': 661,\n '00001820': 662,\n '00001821': 663,\n '00001822': 664,\n '00001823': 665,\n '00001826': 666,\n '00001833': 667,\n '00001839': 668,\n '00001841': 669,\n '00001843': 670,\n '00001845': 671,\n '00001846': 672,\n '00001847': 673,\n '00001848': 674,\n '00001855': 675,\n '00001857': 676,\n '00001858': 677,\n '00001859': 678,\n '00001862': 679,\n '00001863': 680,\n '00001865': 681,\n '00001866': 682,\n '00001870': 683,\n '00001881': 684,\n '00001883': 685,\n '00001886': 686,\n '00001889': 687,\n '00001891': 688,\n '00001893': 689,\n '00001896': 690,\n '00001897': 691,\n '00001902': 692,\n '00001903': 693,\n '00001906': 694,\n '00001909': 695,\n '00001917': 696,\n '00001920': 697,\n '00001921': 698,\n '00001925': 699,\n '00001927': 700,\n '00001929': 701,\n '00001930': 702,\n '00001940': 703,\n '00001944': 704,\n '00001951': 705,\n '00001970': 706,\n '00001971': 707,\n '00001983': 708,\n '00001987': 709,\n '00001988': 710,\n '00001989': 711,\n '00001994': 712,\n '00001997': 713,\n '00002004': 714,\n '00002006': 715,\n '00002008': 716,\n '00002009': 717,\n '00002016': 718,\n '00002019': 719,\n '00002020': 720,\n '00002024': 721,\n '00002033': 722,\n '00002034': 723,\n '00002035': 724,\n '00002038': 725,\n '00002049': 726,\n '00002056': 727,\n '00002057': 728,\n '00002061': 729,\n '00002063': 730,\n '00002066': 731,\n '00002070': 732,\n '00002074': 733,\n '00002075': 734,\n '00002082': 735,\n '00002084': 736,\n '00002087': 737,\n '00002090': 738,\n '00002099': 739,\n '00002101': 740,\n '00002102': 741,\n '00002107': 742,\n '00002110': 743,\n '00002111': 744,\n '00002112': 745,\n '00002113': 746,\n '00002114': 747,\n '00002115': 748,\n '00002118': 749,\n '00002119': 750,\n '00002125': 751,\n '00002126': 752,\n '00002136': 753,\n '00002137': 754,\n '00002141': 755,\n '00002151': 756,\n '00002152': 757,\n '00002159': 758,\n '00002160': 759,\n '00002161': 760,\n '00002164': 761,\n '00002167': 762,\n '00002168': 763,\n '00002170': 764,\n '00002171': 765,\n '00002172': 766,\n '00002176': 767,\n '00002178': 768,\n '00002180': 769,\n '00002185': 770,\n '00002186': 771,\n '00002187': 772,\n '00002188': 773,\n '00002190': 774,\n '00002197': 775,\n '00002198': 776,\n '00002200': 777,\n '00002203': 778,\n '00002206': 779,\n '00002208': 780,\n '00002211': 781,\n '00002213': 782,\n '00002217': 783,\n '00002220': 784,\n '00002221': 785,\n '00002223': 786,\n '00002225': 787,\n '00002227': 788,\n '00002228': 789,\n '00002229': 790,\n '00002230': 791,\n '00002234': 792,\n '00002235': 793,\n '00002240': 794,\n '00002241': 795,\n '00002246': 796,\n '00002247': 797,\n '00002248': 798,\n '00002249': 799,\n '00002252': 800,\n '00002261': 801,\n '00002262': 802,\n '00002263': 803,\n '00002264': 804,\n '00002269': 805,\n '00002274': 806,\n '00002275': 807,\n '00002276': 808,\n '00002277': 809,\n '00002278': 810,\n '00002279': 811,\n '00002280': 812,\n '00002282': 813,\n '00002284': 814,\n '00002288': 815,\n '00002289': 816,\n '00002291': 817,\n '00002294': 818,\n '00002295': 819,\n '00002296': 820,\n '00002297': 821,\n '00002298': 822,\n '00002299': 823,\n '00002300': 824,\n '00002310': 825,\n '00002314': 826,\n '00002315': 827,\n '00002319': 828,\n '00002323': 829,\n '00002324': 830,\n '00002325': 831,\n '00002326': 832,\n '00002328': 833,\n '00002330': 834,\n '00002331': 835,\n '00002332': 836,\n '00002333': 837,\n '00002335': 838,\n '00002337': 839,\n '00002338': 840,\n '00002339': 841,\n '00002340': 842,\n '00002341': 843,\n '00002342': 844,\n '00002343': 845,\n '00002345': 846,\n '00002348': 847,\n '00002350': 848,\n '00002352': 849,\n '00002353': 850,\n '00002355': 851,\n '00002358': 852,\n '00002363': 853,\n '00002364': 854,\n '00002365': 855,\n '00002366': 856,\n '00002367': 857,\n '00002368': 858,\n '00002370': 859,\n '00002378': 860,\n '00002382': 861,\n '00002383': 862,\n '00002384': 863,\n '00002389': 864,\n '00002390': 865,\n '00002391': 866,\n '00002392': 867,\n '00002393': 868,\n '00002394': 869,\n '00002395': 870,\n '00002397': 871,\n '00002398': 872,\n '00002400': 873,\n '00002401': 874,\n '00002405': 875,\n '00002407': 876,\n '00002408': 877,\n '00002412': 878,\n '00002414': 879,\n '00002415': 880,\n '00002420': 881,\n '00002422': 882,\n '00002423': 883,\n '00002424': 884,\n '00002425': 885,\n '00002427': 886,\n '00002430': 887,\n '00002432': 888,\n '00002435': 889,\n '00002436': 890,\n '00002440': 891,\n '00002441': 892,\n '00002442': 893,\n '00002443': 894,\n '00002444': 895,\n '00002449': 896,\n '00002452': 897,\n '00002453': 898,\n '00002454': 899,\n '00002462': 900,\n '00002464': 901,\n '00002466': 902,\n '00002468': 903,\n '00002472': 904,\n '00002473': 905,\n '00002494': 906,\n '00002495': 907,\n '00002496': 908,\n '00002497': 909,\n '00002501': 910,\n '00002502': 911,\n '00002509': 912,\n '00002528': 913,\n '00002529': 914,\n '00002533': 915,\n '00002537': 916,\n '00002538': 917,\n '00002540': 918,\n '00002544': 919,\n '00002547': 920,\n '00002549': 921,\n '00002555': 922,\n '00002562': 923,\n '00002569': 924,\n '00002578': 925,\n '00002582': 926,\n '00002583': 927,\n '00002590': 928,\n '00002591': 929,\n '00002595': 930,\n '00002596': 931,\n '00002597': 932,\n '00002598': 933,\n '00002600': 934,\n '00002601': 935,\n '00002602': 936,\n '00002608': 937,\n '00002609': 938,\n '00002611': 939,\n '00002612': 940,\n '00002613': 941,\n '00002616': 942,\n '00002618': 943,\n '00002619': 944,\n '00002620': 945,\n '00002621': 946,\n '00002623': 947,\n '00002625': 948,\n '00002626': 949,\n '00002627': 950,\n '00002628': 951,\n '00002629': 952,\n '00002630': 953,\n '00002632': 954,\n '00002634': 955,\n '00002635': 956,\n '00002636': 957,\n '00002637': 958,\n '00002638': 959,\n '00002639': 960,\n '00002640': 961,\n '00002645': 962,\n '00002646': 963,\n '00002647': 964,\n '00002648': 965,\n '00002649': 966,\n '00002652': 967,\n '00002653': 968,\n '00002654': 969,\n '00002655': 970,\n '00002666': 971,\n '00002667': 972,\n '00002668': 973,\n '00002669': 974,\n '00002674': 975,\n '00002680': 976,\n '00002681': 977,\n '00002684': 978,\n '00002685': 979,\n '00002687': 980,\n '00002688': 981,\n '00002700': 982,\n '00002702': 983,\n '00002704': 984,\n '00002705': 985,\n '00002706': 986,\n '00002707': 987,\n '00002709': 988,\n '00002714': 989,\n '00002716': 990,\n '00002717': 991,\n '00002724': 992,\n '00002725': 993,\n '00002730': 994,\n '00002732': 995,\n '00002733': 996,\n '00002734': 997,\n '00002738': 998,\n '00002745': 999,\n '00002746': 1000,\n ...}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_tokenizer(item_code):\n",
    "    \"\"\"\n",
    "    Create a tokenizer to tokenize item codes.\n",
    "\n",
    "    Args:\n",
    "        item_code (list or Series): List or Series containing item codes.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.preprocessing.text.Tokenizer: Tokenizer object fitted on item codes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a tokenizer with no filters and case-sensitive tokenization\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', lower=False)\n",
    "\n",
    "    # Fit the tokenizer on the item codes\n",
    "    tokenizer.fit_on_texts(item_code)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "# Create a tokenizer using item codes from item_sales dataframe columns\n",
    "tokenizer = create_tokenizer(item_sales.columns[6:].str.replace(' ', ''))\n",
    "\n",
    "# Get the length of the tokenizer's word index\n",
    "tokenizer_word_count = len(tokenizer.word_index)\n",
    "\n",
    "print(f'Tokenizer has {tokenizer_word_count} tokens')\n",
    "tokenizer.word_index"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "epIOoC6PeLLI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686287226769,
     "user_tz": -420,
     "elapsed": 15,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "3a27cb81-ce45-4691-f648-7a5ca46f2095"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have the tokenizer, we can create the input and output data for the model."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T06:00:48.499412900Z",
     "start_time": "2023-06-11T06:00:48.326983200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([1., 3., 0., ..., 0., 0., 0.])"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start with the input data.\n",
    "Since we are using item codes as a feature, month, day, day of the week and day of the year, we need to create prefix features for each item code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "430"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract date values from item_sales dataframe columns\n",
    "dates = np.array(item_sales[['month', 'day', 'day_of_week', 'day_of_year']])\n",
    "\n",
    "# Perform cyclic encoding on the date values\n",
    "dates_cyclic = np.sin(dates) + np.cos(dates)\n",
    "\n",
    "prefix_features = []\n",
    "\n",
    "# Iterate over each product in item_sales columns\n",
    "for product in item_sales.columns[6:].str.replace(' ', ''):\n",
    "    # Create prefix features for the product\n",
    "    prefix_feature = np.array([\n",
    "        [\n",
    "            tokenizer.word_index[product],\n",
    "            dates_cyclic[j][0],\n",
    "            dates_cyclic[j][1],\n",
    "            dates_cyclic[j][2],\n",
    "            dates_cyclic[j][3]\n",
    "        ]\n",
    "        for j in range(WINDOW, len(item_sales))\n",
    "    ], dtype=np.float64)\n",
    "    \n",
    "    prefix_features.append(prefix_feature)\n",
    "\n",
    "# Get the total number of prefix features arrays and the shape of the first array\n",
    "prefix_features_count = len(prefix_features)\n",
    "prefix_features_shape = prefix_features[0].shape\n",
    "\n",
    "print(f\"A total of {prefix_features_count} numpy arrays with each one having shape {prefix_features_shape}\")\n",
    "\n",
    "print(prefix_features[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ojZPbyzjeLLE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686287232903,
     "user_tz": -420,
     "elapsed": 6146,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "105b1cce-2c70-4202-8df4-0cd4b8bbe6d7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We not normalize the sales data in order to process later."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create a list of tensorflow datasets, one for each item code. This will be used to create the windowed dataset using the Dataset API."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(5984310, 4)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#convert each item sales to a tensorflow dataset\n",
    "sales_datasets = [tf.data.Dataset.from_tensor_slices(item_sales[column]) for column in\n",
    "sales_datasets[0].element_spec"
   ],
   "metadata": {
    "id": "G-yzQnnHeLLF",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686287281623,
     "user_tz": -420,
     "elapsed": 23741,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "88bebf73-dd69-423a-9c9a-2f373ad9dba7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def window_dataset(token_time_ds, sales_ds, window_size):\n",
    "    \"\"\"\n",
    "    Create a windowed dataset by combining token_time_ds and sales_ds.\n",
    "\n",
    "    Args:\n",
    "        token_time_ds (tf.data.Dataset): Dataset containing token and time information.\n",
    "        sales_ds (tf.data.Dataset): Dataset containing sales information.\n",
    "        window_size (int): Size of the window for creating sequences.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Windowed dataset with input-output pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create windows of size window_size+1, shifting by 1, and dropping any incomplete windows\n",
    "    sales_ds = sales_ds.window(window_size+1, shift=1, drop_remainder=True)\n",
    "\n",
    "    # Flatten the windows into individual datasets and combine them into a single dataset\n",
    "    sales_ds = sales_ds.flat_map(lambda w: w.batch(window_size+1))\n",
    "\n",
    "    # Concatenate token_time_ds and sales_ds tensors along the last axis\n",
    "    windowed_tensors = tf.concat((list(token_time_ds), list(sales_ds)), axis=-1)\n",
    "\n",
    "    # Create a new dataset from the concatenated tensors\n",
    "    ds = tf.data.Dataset.from_tensor_slices(windowed_tensors)\n",
    "\n",
    "    # Map each element of the dataset to input-output pairs\n",
    "    ds = ds.map(lambda x: (x[:-1], x[-1]),num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    return ds\n"
   ],
   "metadata": {
    "id": "R75X6KAVeLLG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686287281624,
     "user_tz": -420,
     "elapsed": 25,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create a windowed dataset for each item code. We use a window size of 20 days.\n",
    "We clean up the memory by deleting the dataframes and series that are no longer needed. This is needed to avoid running out of memory."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#window the dataset in batches\n",
    "ds = [window_dataset(prefix_features[i],sales_datasets[i], WINDOW) for i in range(len(sales_datasets))]\n",
    "del data\n",
    "del prefix_features\n",
    "del sales_datasets\n",
    "del item_sales"
   ],
   "metadata": {
    "id": "PuQNPgFAeLLH",
    "executionInfo": {
     "status": "error",
     "timestamp": 1686287808730,
     "user_tz": -420,
     "elapsed": 527130,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "outputId": "f1d60b1b-662b-44bf-8a0d-6476f0164d9b"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We split the dataset into training, validation, and test partitions."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((5984310, 20), (5984310,))"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "def get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=False, shuffle_size=1000):\n",
    "    \"\"\"\n",
    "    Splits a TensorFlow dataset into training, validation, and test partitions.\n",
    "\n",
    "    Args:\n",
    "        ds (tf.data.Dataset): The input dataset.\n",
    "        ds_size (int): The total size of the input dataset.\n",
    "        train_split (float, optional): The fraction of data to allocate for training. Defaults to 0.8.\n",
    "        val_split (float, optional): The fraction of data to allocate for validation. Defaults to 0.1.\n",
    "        test_split (float, optional): The fraction of data to allocate for testing. Defaults to 0.1.\n",
    "        shuffle (bool, optional): Whether to shuffle the training dataset. Defaults to True.\n",
    "        shuffle_size (int, optional): The buffer size used for shuffling. Defaults to 10000.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the training, validation, and test partitions of the dataset.\n",
    "    \"\"\"\n",
    "    assert (train_split + test_split + val_split) == 1, \"The sum of train_split, val_split, and test_split must be 1.\"\n",
    "    \n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    if shuffle:\n",
    "      # Specify seed to always have the same split distribution between runs\n",
    "      train_ds = ds.take(train_size).shuffle(shuffle_size, seed=12)\n",
    "    else:\n",
    "      train_ds = ds.take(train_size)\n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds\n"
   ],
   "metadata": {
    "id": "0aqw4tpeaayW",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1686287808733,
     "user_tz": -420,
     "elapsed": 16,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 13:17:49.746760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-11 13:17:49.758746: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-11 13:17:49.760391: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-11 13:17:49.762823: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-11 13:17:49.764979: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-11 13:17:49.766648: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-11 13:17:49.768291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-11 13:17:50.550859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-11 13:17:50.552790: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-11 13:17:50.554523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-11 13:17:50.556120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13582 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#create a normalization layer for input data\u001B[39;00m\n\u001B[1;32m      2\u001B[0m normalizer \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mNormalization(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m \u001B[43mnormalizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madapt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwindowed_input\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m max_out \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmax(windowed_output)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m#normalize the output data\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/keras/layers/preprocessing/normalization.py:286\u001B[0m, in \u001B[0;36mNormalization.adapt\u001B[0;34m(self, data, batch_size, steps)\u001B[0m\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madapt\u001B[39m(\u001B[38;5;28mself\u001B[39m, data, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    241\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Computes the mean and variance of values in a dataset.\u001B[39;00m\n\u001B[1;32m    242\u001B[0m \n\u001B[1;32m    243\u001B[0m \u001B[38;5;124;03m    Calling `adapt()` on a `Normalization` layer is an alternative to\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;124;03m          argument is not supported with array inputs.\u001B[39;00m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 286\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madapt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/base_preprocessing_layer.py:258\u001B[0m, in \u001B[0;36mPreprocessingLayer.adapt\u001B[0;34m(self, data, batch_size, steps)\u001B[0m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mcatch_stop_iteration():\n\u001B[1;32m    257\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39msteps():\n\u001B[0;32m--> 258\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_adapt_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    259\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[1;32m    260\u001B[0m             context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    877\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    879\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 880\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    882\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    883\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    909\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    910\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m    911\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[0;32m--> 912\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_no_variable_creation_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[1;32m    913\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    914\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[1;32m    915\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[1;32m    916\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001B[0m, in \u001B[0;36mTracingCompiler.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    132\u001B[0m   (concrete_function,\n\u001B[1;32m    133\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[0;32m--> 134\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconcrete_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    135\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconcrete_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1741\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1743\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1744\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1745\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1746\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1747\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1748\u001B[0m     args,\n\u001B[1;32m   1749\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1750\u001B[0m     executing_eagerly)\n\u001B[1;32m   1751\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:341\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    337\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    338\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstateful_ops\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    339\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_ops\n\u001B[0;32m--> 341\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall\u001B[39m(\u001B[38;5;28mself\u001B[39m, ctx, args, cancellation_manager\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    342\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Calls this function with `args` as inputs.\u001B[39;00m\n\u001B[1;32m    343\u001B[0m \n\u001B[1;32m    344\u001B[0m \u001B[38;5;124;03m  `ConcreteFunction` execution respects device annotations only if the\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    359\u001B[0m \u001B[38;5;124;03m      available to be called because it has been garbage collected.\u001B[39;00m\n\u001B[1;32m    360\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[1;32m    361\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39minput_arg):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#split the datasets to train, val, test\n",
    "ds = [get_dataset_partitions_tf(items, 431, shuffle_size=BUFFER) for items in ds]"
   ],
   "metadata": {
    "id": "fZwJAsF3ojX7",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1686287808735,
     "user_tz": -420,
     "elapsed": 17,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#split into individual sets\n",
    "train_set = [ds[i][0] for i in range(len(ds))]\n",
    "val_set = [ds[i][1] for i in range(len(ds))]\n",
    "test_set = [ds[i][2]for i in range(len(ds))]"
   ],
   "metadata": {
    "id": "sjITHB8MyfPi",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1686287808736,
     "user_tz": -420,
     "elapsed": 18,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#turn into tensors\n",
    "train_set = tf.data.experimental.from_list(train_set).flat_map(lambda x: x)\n",
    "val_set = tf.data.experimental.from_list(val_set).flat_map(lambda x: x)\n",
    "test_set = tf.data.experimental.from_list(test_set).flat_map(lambda x: x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need the length of each set for sampling later.\n",
    "**Note:**\n",
    "This process takes a long time. It's better once you have the length of each set to save it to a file and load it later."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#find the length of each set using map\n",
    "train_len = train_set.reduce(0, lambda x, _: x + 1).numpy()\n",
    "val_len = val_set.reduce(0, lambda x, _: x + 1).numpy()\n",
    "test_len = test_set.reduce(0, lambda x, _: x + 1).numpy()\n",
    "# train_len = 4787448\n",
    "# val_len = 598388\n",
    "# test_len = 612348\n",
    "\n",
    "print(f\"Training set length: {train_len}\")\n",
    "print(f\"Validation set length: {val_len}\")\n",
    "print(f\"Test set length: {test_len}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#batch and prefetch the datasets\n",
    "train_set = train_set.shuffle(BUFFER).batch(BATCH_SIZE).prefetch(1)\n",
    "val_set = val_set.batch(BATCH_SIZE).prefetch(1)\n",
    "test_set = test_set.batch(BATCH_SIZE).prefetch(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We save the datasets to disk so we can load them later. This is needed because the process of creating the datasets takes a long time."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#save the datasets\n",
    "tf.data.Dataset.save(train_set, 'train_set')\n",
    "tf.data.Dataset.save(val_set, 'val_set')\n",
    "tf.data.Dataset.save(test_set, 'test_set')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "After preparing the dataset, we try to fine tune the learning rate of the algorithm.\n",
    "We only use a sample of the training set to speed up the process."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T04:08:58.168373800Z",
     "start_time": "2023-06-11T04:08:58.124552200Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#create a list of models based on given learning rates and optimizers\n",
    "def create_model():\n",
    "    \"\"\"\n",
    "    Creates a list of models based on the learning rates and optimizers given.\n",
    "\n",
    "    Args:\n",
    "        learning_rate_array (list): A list of learning rates and optimizers to use for each model.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of models.\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(filters=64, kernel_size=3,\n",
    "                               strides=1,\n",
    "                               activation=\"relu\", padding=\"causal\",\n",
    "                               input_shape=[25, 1]),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "train_sample = train_set.unbatch().shuffle(BUFFER).take(train_len//10).batch(BATCH_SIZE).prefetch(1)\n",
    "val_sample = val_set.unbatch().shuffle(BUFFER).take(val_len//10).batch(BATCH_SIZE).prefetch(1)\n",
    "train_sample"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#callback to tune the learning rate\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(momentum=0.9)\n",
    "\n",
    "model.compile(loss=tf.keras.losses.Huber(),\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"mape\"])\n",
    "\n",
    "history = model.fit(train_sample, epochs=100, callbacks=[lr_schedule],validation_data = val_sample, verbose=2)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the learning rate array\n",
    "lrs = 1e-8 * (10 ** (np.arange(100) / 20))\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Set the grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot the loss in log scale\n",
    "plt.semilogx(lrs, history.history[\"loss\"])\n",
    "\n",
    "# Increase the tickmarks size\n",
    "plt.tick_params('both', length=10, width=1, which='both')\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We found that the best learning rate is 10e-5 (or 1e-4)."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Reset the states generated by keras\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = create_model()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#set the learning rate\n",
    "learning_rate = 1e-4\n",
    "#set the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "#set the callback to stop the training if the validation loss doesn't improve\n",
    "callback = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "model.compile(loss=tf.keras.losses.Huber(),\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"mape\"])\n",
    "\n",
    "history = model.fit(train_set, epochs=500, validation_data = val_set, verbose=2, callbacks=[callback])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluation = model.evaluate(test_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "accelerator": "TPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
